# Contributing to Multilingual Evaluations

Lighteval supports multilingual evaluations through a comprehensive system of translation literals and language-adapted templates. This guide explains how to contribute translations and create new multilingual tasks.

## Contributing Translation Literals

### What Are Translation Literals?

We define 19 `literals`, basic keywords or punctuation signs used when creating evaluation prompts in an automatic manner, such as `yes`, `no`, `because`, etc.

These literals are essential for:
- **Consistent prompt formatting** across languages
- **Automatic prompt generation** for multilingual tasks
- **Proper localization** of evaluation templates

### How to Contribute Translations

We welcome translations in your language! To contribute:

1. **Open the translation literals file**: [translation_literals.py](https://github.com/huggingface/lighteval/blob/main/src/lighteval/tasks/templates/utils/translation_literals.py)

2. **Edit the file** to add or expand the literal for your language of interest

3. **Open a PR** with your modifications

### Translation Literals Structure

```python
Language.ENGLISH: TranslationLiterals(
    language=Language.ENGLISH,
    question_word="question",  # Usage: "Question: How are you?"
    answer="answer",  # Usage: "Answer: I am fine"
    confirmation_word="right",  # Usage: "He is smart, right?"
    yes="yes",  # Usage: "Yes, he is"
    no="no",  # Usage: "No, he is not"
    also="also",  # Usage: "Also, she is smart."
    cause_word="because",  # Usage: "She is smart, because she is tall"
    effect_word="therefore",  # Usage: "He is tall therefore he is smart"
    or_word="or",  # Usage: "He is tall or small"
    true="true",  # Usage: "He is smart, true, false or neither?"
    false="false",  # Usage: "He is smart, true, false or neither?"
    neither="neither",  # Usage: "He is smart, true, false or neither?"
    # Punctuation and spacing: only adjust if your language uses something different than in English
    full_stop=".",
    comma=",",
    question_mark="?",
    exclamation_mark="!",
    word_space=" ",
    sentence_space=" ",
    colon=":",
    # The first characters of your alphabet used in enumerations, if different from English
    indices=["A", "B", "C", ...]
)
```

### Translation Guidelines

- **Accuracy**: Ensure translations are accurate and contextually appropriate
- **Consistency**: Use consistent terminology across all literals
- **Cultural Sensitivity**: Consider cultural differences in language usage
- **Punctuation**: Only adjust punctuation if your language uses different conventions
- **Indices**: Update the indices list if your language uses different alphabetical characters

## Contributing New Multilingual Tasks

### Prerequisites

Before creating a new multilingual task, you should:

1. **Read the custom task guide**: [Adding a Custom Task](adding-a-custom-task)
2. **Understand multilingual task structure**: Review the [multilingual tasks](https://github.com/huggingface/lighteval/blob/main/src/lighteval/tasks/multilingual/tasks.py) file
3. **Browse available templates**: Check the [templates directory](https://github.com/huggingface/lighteval/tree/main/src/lighteval/tasks/templates)

### Key Concepts

#### Language-Adapted Templates
For multilingual evaluations, the `prompt_function` should be implemented using language-adapted templates. These templates handle:
- **Correct formatting** for each language
- **Consistent usage** of language-adjusted prompt anchors (e.g., Question/Answer)
- **Proper punctuation** and spacing conventions

#### Template Types
Available template types include:
- **XNLI**: Natural language inference tasks
- **COPA**: Causal reasoning tasks
- **Multiple Choice**: Standard multiple choice questions
- **Question Answering**: Open-ended question answering
- **Custom**: Specialized task templates

### Creating Your Multilingual Task

#### Step 1: Create the Task File
Create a Python file following the custom task guide structure.

#### Step 2: Import Required Components
```python
from lighteval.tasks.lighteval_task import LightevalTaskConfig
from lighteval.tasks.multilingual.language import Language
from lighteval.tasks.multilingual.formulations import MCFFormulation, CFFormulation, HybridFormulation
from lighteval.tasks.multilingual.templates import get_template_prompt_function
from lighteval.tasks.multilingual.metrics import get_metrics_for_formulation, loglikelihood_acc_metric
from lighteval.tasks.multilingual.normalization import LogProbTokenNorm, LogProbCharNorm
```

#### Step 3: Define Your Tasks
```python
your_tasks = [
    LightevalTaskConfig(
        # Name of your evaluation
        name=f"evalname_{language.value}_{formulation.name.lower()}",
        # The evaluation is community contributed
        suite=["community"],
        # This will automatically get the correct metrics for your chosen formulation
        metric=get_metrics_for_formulation(
            formulation,
            [
                LogLikelihoodAccMetric(normalization=None),
                LogLikelihoodAccMetric(normalization=LogProbTokenNorm()),
                LogLikelihoodAccMetric(normalization=LogProbCharNorm()),
            ],
        ),
        # In this function, you choose which template to follow and for which language and formulation
        prompt_function=get_template_prompt_function(
            language=language,
            # Use the adapter to define the mapping between the
            # keys of the template (left), and the keys of your dataset
            # (right)
            # To know which template keys are required and available,
            # consult the appropriate adapter type and doc-string.
            adapter=lambda line: {
                "key": line["relevant_key"],
                # Add more mappings as needed
            },
            formulation=formulation,
        ),
        # You can also add specific filters to remove irrelevant samples
        hf_filter=lambda line: line["label"] in <condition>,
        # You then select your huggingface dataset as well as
        # the splits available for evaluation
        hf_repo=<dataset>,
        hf_subset=<subset>,
        evaluation_splits=["train"],
        hf_avail_splits=["train"],
    )
    for language in [
        Language.YOUR_LANGUAGE,  # Add your target languages
        # Language.SPANISH,
        # Language.FRENCH,
        # etc.
    ]
    for formulation in [MCFFormulation(), CFFormulation(), HybridFormulation()]
]
```

#### Step 4: Test Your Implementation
Follow the custom task guide to test if your task is correctly implemented.

> [!TIP]
> All [`~tasks.lighteval_task.LightevalTaskConfig`] parameters are strongly typed, including the inputs to the template function. Make sure to take advantage of your IDE's functionality to make it easier to correctly fill these parameters.

### Formulation Types

#### Multiple Choice Formulation (MCF)
Used for standard multiple choice questions:
```python
MCFFormulation()
```

#### Classification Formulation (CF)
Used for classification tasks:
```python
CFFormulation()
```

#### Hybrid Formulation
Used for tasks that combine multiple approaches:
```python
HybridFormulation()
```

### Language Support

Currently supported languages include:
- **English** (ENGLISH)
- **Spanish** (SPANISH)
- **French** (FRENCH)
- **German** (GERMAN)
- **Italian** (ITALIAN)
- **Portuguese** (PORTUGUESE)
- **Russian** (RUSSIAN)
- **Chinese** (CHINESE)
- **Japanese** (JAPANESE)
- **Korean** (KOREAN)
- **Arabic** (ARABIC)
- **Hindi** (HINDI)
- **Turkish** (TURKISH)
- **Dutch** (DUTCH)
- **Polish** (POLISH)
- **Swedish** (SWEDISH)
- **Norwegian** (NORWEGIAN)
- **Danish** (DANISH)
- **Finnish** (FINNISH)

### Best Practices

#### Translation Literals
- **Test your translations** with native speakers
- **Consider regional variations** in language usage
- **Maintain consistency** with existing translations
- **Document any special considerations** in your PR

#### Multilingual Tasks
- **Choose appropriate templates** for your task type
- **Test across multiple languages** to ensure consistency
- **Consider cultural differences** in task interpretation
- **Validate dataset quality** for each language
- **Use appropriate metrics** for your task formulation

#### Code Quality
- **Follow existing code style** and conventions
- **Add comprehensive documentation** for your task
- **Include example usage** in your PR description
- **Test thoroughly** before submitting

### Testing Your Contribution

#### Local Testing
```bash
# Test your task locally
lighteval accelerate \
    "model_name=openai-community/gpt2" \
    "community|your_task_name|0|0" \
    --max-samples 10
```

#### Validation Checklist
- [ ] Translation literals are accurate and complete
- [ ] Task works correctly across all target languages
- [ ] Metrics are appropriate for the task type
- [ ] Documentation is clear and comprehensive
- [ ] Code follows project conventions

### Submitting Your Contribution

1. **Create a feature branch** for your changes
2. **Add comprehensive tests** for your task
3. **Update documentation** as needed
4. **Open a pull request** with a clear description
5. **Respond to review feedback** promptly

Once everything is good, open a PR, and we'll be happy to review it!

### Getting Help

- **GitHub Issues**: Report bugs or ask questions
- **Discussions**: Join community discussions
- **Documentation**: Review existing guides and examples

For more detailed information about multilingual evaluations, see the [Multilingual Tasks Reference](package_reference/tasks).
