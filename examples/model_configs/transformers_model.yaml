model_parameters:
  model_name: "meta-llama/Llama-3.1-8B-Instruct"
  revision: "main"
  dtype: "float16"
  compile: false
  model_parallel: false
  batch_size: 20
  use_chat_template: true
  continuous_batching: false
  model_loading_kwargs:
    attn_implementation: "flash_attention_2"
    #tp_plan: "auto"
  generation_parameters:
    #num_blocks: 4096
    #block_size: 64
    max_new_tokens: 256
    temperature: 0.2
