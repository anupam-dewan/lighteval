model_parameters:
  model_name: "meta-llama/Llama-3.1-8B-Instruct"
  revision: "main"
  dtype: "float16"
  compile: false
  model_parallel: false
  batch_size: 10
  use_chat_template: true
  continuous_batching: true
  model_loading_kwargs:
    attn_implementation: "paged_attention"
    #tp_plan: "auto"
  generation_parameters:
    num_blocks: 4096
    block_size: 64
    max_new_tokens: 256
    temperature: 0.2
